{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "#### In this notebook we'll show the results of different preprocessing steps used on the data to predict falls using SVM models and Convolutional Neural Network models. \n",
    "\n",
    "#### SVMs were picked due to being the best model used to predict falls in research that uses similar types of data as us (sensor data instead of images or video).  The idea behind CNNs is that, if the data has some kind of pattern in terms of peak velocities or accelerations from different sensors, then CNNs will learn this better and faster than other models. In addition, we couldn't find any research on the use of CNNs for fall detection that wasn't time series analysis, so we decided to give it a try.\n",
    "\n",
    "#### The notebook will be divided in 4 parts, each one using a different set of preprocessing steps. The first 3 use SVMs as the predictive model and the last one uses CNNs. The reason for this is that there is no clear definition on what a \"fall\" is, different research papers consider a \"fall\" as something different, everyone agrees that it starts with a loss of balance then a peak in acceleration, and at the end the acceleration and velocity stop when the person hits the ground, the problem is identifying this with sensor data. Some researchers use a X second window around the peak acceleration, for example.  We wanted to focus more on the effects of these preprocessing steps on the predictions as well as documenting these steps correctly, since we've found that most other researchers don't explain their preprocessing steps in a detailed manner.\n",
    "\n",
    "#### These steps involve mostly the definition of a \"fall\", as in, with these we'll set the targets of our machine learning algorithms  as fall or not fall (1 or 0) as well as pick or drop the columns of data that our models will use. Each section will have an explanation of these steps.\n",
    "\n",
    "#### The models in this notebook use the set of hyperparameters that we found produced the best results.  Additionally, keep in mind that of the trials, and in those trials, about one third of the trial itself is an actual fall. Due to this, it's fairly easy to get high accuracy but not so easy to do a good job at actually predicting the falls themselves.\n",
    "\n",
    "#### Additionally there is one last section for visualization on the results of different tests that we performed. It was done this way to avoid having to rerun the different parts of the program each time since some of them take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob, os\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D,MaxPooling1D, Flatten,Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adds the target as 0s for ADLs and NearFalls and 1s for Falls\n",
    "def generateTarget(row) :\n",
    "    if row['trial_type'] == 'ADLs' :\n",
    "        return 0\n",
    "    if row['trial_type'] == 'Near_Falls' :\n",
    "        return 0\n",
    "    if row['trial_type'] == 'Falls' :\n",
    "        return 1\n",
    "    \n",
    "# Adds the target as 0s for every column (to add the 1s later)\n",
    "def generateTarget2(row) :\n",
    "    return 0\n",
    "\n",
    "# Function used to test the models except the CNNs since those have a different syntax\n",
    "def modelProcessing(X_train,y_train,X_test,y_test,model) :\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "    specificity = (tn / (fp + tn))*100\n",
    "    sensitivity = (tp / (tp + fn))*100\n",
    "    accuracy = ((tn+tp) / (tp + tn + fp + fn))\n",
    "    print(\"Confusion matrix : \")\n",
    "    print(\"TN : \"+str(tn) + \" FP : \" +str(fp))\n",
    "    print(\"FN : \"+str(fn) + \" TP : \" +str(tp))\n",
    "    print(\"\")\n",
    "    print(\"Accuracy : \"+str(accuracy_score(y_test,y_pred)))\n",
    "    print(\"Recall : \" +str(recall_score(y_test,y_pred)))\n",
    "    print(\"Precision : \"+str(precision_score(y_test,y_pred)))\n",
    "    print(\"F-measure :\"+str(f1_score(y_test,y_pred)))\n",
    "    print(\"Sensitivity : \"+str(sensitivity))\n",
    "    print(\"Specificity : \"+str(specificity))\n",
    "    \n",
    "\n",
    "# Obtain a DF with the metrics and bodyparts you want\n",
    "def filterCols(df,metrics,bodyparts,resultants=True) :\n",
    "    # Metrics are Acceleration,Magnetic and Velocity (List of strings)\n",
    "    # Bodyparts are waist,l.ankle,r.ankle,l.thigh,r.thigh,sternum,head (list of strings)\n",
    "    # Resultants = True will get the resultants of the respective metrics\n",
    "    groupcols = ['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime']\n",
    "    metriccols = []\n",
    "    bodycols = []\n",
    "    \n",
    "    for col in df.columns.values :\n",
    "        for metric in metrics :\n",
    "            if (metric in col) :\n",
    "                metriccols.append(col)\n",
    "            if (resultants) :\n",
    "                if (metric.lower() in col) :\n",
    "                    metriccols.append(col)\n",
    "        for part in bodyparts :\n",
    "            if (part in col) :\n",
    "                bodycols.append(col)\n",
    "    dfOut = df[groupcols + list(set(metriccols) & set(bodycols))]\n",
    "    return dfOut\n",
    "\n",
    "# Plots the results of a tested model with a multi-bar plot\n",
    "def plotRes(accL,recL,precL,specL,xticks) :\n",
    "    x = np.array([i for i in range(0,len(specL))])\n",
    "    ax = plt.subplot(111)\n",
    "    plt.xticks(x, xticks)\n",
    "    bar1 = ax.bar(x-0.2, accL,width=0.1,color='b',align='center',label='acc')\n",
    "    bar2 = ax.bar(x-0.1, precL,width=0.1,color='g',align='center',label='prec')\n",
    "    bar3 = ax.bar(x, recL,width=0.1,color='r',align='center',label='rec')\n",
    "    bar4 = ax.bar(x+0.1, specL,width=0.1,color='y',align='center',label='spec')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=[bar1,bar2,bar3,bar4],loc='upper center',bbox_to_anchor=(1.1,1.0))\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>time_datetime</th>\n",
       "      <th>time_seconds</th>\n",
       "      <th>subject</th>\n",
       "      <th>trial_type</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>trial_num_original</th>\n",
       "      <th>trial_subtype</th>\n",
       "      <th>r.ankle Acceleration X (m/s^2)</th>\n",
       "      <th>r.ankle Acceleration Y (m/s^2)</th>\n",
       "      <th>...</th>\n",
       "      <th>head resultant angular velocity</th>\n",
       "      <th>head resultant magnetic field</th>\n",
       "      <th>sternum resultant acceleration</th>\n",
       "      <th>sternum resultant angular velocity</th>\n",
       "      <th>sternum resultant magnetic field</th>\n",
       "      <th>waist resultant acceleration</th>\n",
       "      <th>waist resultant angular velocity</th>\n",
       "      <th>waist resultant magnetic field</th>\n",
       "      <th>FileName</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1304799112429680</td>\n",
       "      <td>2011-05-07 20:11:52.429680</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.691464</td>\n",
       "      <td>-0.240769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205785</td>\n",
       "      <td>83.535679</td>\n",
       "      <td>9.735780</td>\n",
       "      <td>0.265283</td>\n",
       "      <td>50.703732</td>\n",
       "      <td>9.864789</td>\n",
       "      <td>1.122888</td>\n",
       "      <td>57.649936</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1304799112437500</td>\n",
       "      <td>2011-05-07 20:11:52.437500</td>\n",
       "      <td>0.00782</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.698196</td>\n",
       "      <td>-0.245077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215989</td>\n",
       "      <td>83.284594</td>\n",
       "      <td>9.736724</td>\n",
       "      <td>0.272279</td>\n",
       "      <td>46.261673</td>\n",
       "      <td>9.845113</td>\n",
       "      <td>1.129150</td>\n",
       "      <td>53.942026</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1304799112445310</td>\n",
       "      <td>2011-05-07 20:11:52.445310</td>\n",
       "      <td>0.01563</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.707324</td>\n",
       "      <td>-0.240524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215544</td>\n",
       "      <td>84.138524</td>\n",
       "      <td>9.748169</td>\n",
       "      <td>0.269626</td>\n",
       "      <td>51.684996</td>\n",
       "      <td>9.843324</td>\n",
       "      <td>1.121401</td>\n",
       "      <td>49.053275</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1304799112453120</td>\n",
       "      <td>2011-05-07 20:11:52.453120</td>\n",
       "      <td>0.02344</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.700565</td>\n",
       "      <td>-0.238417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208668</td>\n",
       "      <td>82.744759</td>\n",
       "      <td>9.746346</td>\n",
       "      <td>0.266867</td>\n",
       "      <td>52.142097</td>\n",
       "      <td>9.833984</td>\n",
       "      <td>1.125120</td>\n",
       "      <td>56.348231</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1304799112460930</td>\n",
       "      <td>2011-05-07 20:11:52.460930</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.698231</td>\n",
       "      <td>-0.242868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225453</td>\n",
       "      <td>82.618975</td>\n",
       "      <td>9.738551</td>\n",
       "      <td>0.269698</td>\n",
       "      <td>55.118507</td>\n",
       "      <td>9.841990</td>\n",
       "      <td>1.133976</td>\n",
       "      <td>55.478944</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1304799112468750</td>\n",
       "      <td>2011-05-07 20:11:52.468750</td>\n",
       "      <td>0.03907</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.698263</td>\n",
       "      <td>-0.240662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218195</td>\n",
       "      <td>81.709223</td>\n",
       "      <td>9.736395</td>\n",
       "      <td>0.279396</td>\n",
       "      <td>46.261673</td>\n",
       "      <td>9.844404</td>\n",
       "      <td>1.121709</td>\n",
       "      <td>57.873096</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1304799112476560</td>\n",
       "      <td>2011-05-07 20:11:52.476560</td>\n",
       "      <td>0.04688</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.705055</td>\n",
       "      <td>-0.240563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219443</td>\n",
       "      <td>80.866421</td>\n",
       "      <td>9.731590</td>\n",
       "      <td>0.274831</td>\n",
       "      <td>46.207206</td>\n",
       "      <td>9.836417</td>\n",
       "      <td>1.123687</td>\n",
       "      <td>60.276311</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1304799112484370</td>\n",
       "      <td>2011-05-07 20:11:52.484370</td>\n",
       "      <td>0.05469</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.702853</td>\n",
       "      <td>-0.236186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216588</td>\n",
       "      <td>82.462735</td>\n",
       "      <td>9.738219</td>\n",
       "      <td>0.279047</td>\n",
       "      <td>47.141498</td>\n",
       "      <td>9.845819</td>\n",
       "      <td>1.111922</td>\n",
       "      <td>52.042055</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1304799112492180</td>\n",
       "      <td>2011-05-07 20:11:52.492180</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.700576</td>\n",
       "      <td>-0.236234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218812</td>\n",
       "      <td>82.890114</td>\n",
       "      <td>9.741219</td>\n",
       "      <td>0.266923</td>\n",
       "      <td>58.814823</td>\n",
       "      <td>9.846862</td>\n",
       "      <td>1.124236</td>\n",
       "      <td>53.510003</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1304799112500000</td>\n",
       "      <td>2011-05-07 20:11:52.500000</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>1</td>\n",
       "      <td>ADLs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>-9.691576</td>\n",
       "      <td>-0.234133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217349</td>\n",
       "      <td>82.744759</td>\n",
       "      <td>9.732827</td>\n",
       "      <td>0.274406</td>\n",
       "      <td>53.075897</td>\n",
       "      <td>9.861802</td>\n",
       "      <td>1.122197</td>\n",
       "      <td>53.643669</td>\n",
       "      <td>JXL_SQ_trial1.xlsx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time              time_datetime  time_seconds  subject  \\\n",
       "0  1304799112429680 2011-05-07 20:11:52.429680       0.00000        1   \n",
       "1  1304799112437500 2011-05-07 20:11:52.437500       0.00782        1   \n",
       "2  1304799112445310 2011-05-07 20:11:52.445310       0.01563        1   \n",
       "3  1304799112453120 2011-05-07 20:11:52.453120       0.02344        1   \n",
       "4  1304799112460930 2011-05-07 20:11:52.460930       0.03125        1   \n",
       "5  1304799112468750 2011-05-07 20:11:52.468750       0.03907        1   \n",
       "6  1304799112476560 2011-05-07 20:11:52.476560       0.04688        1   \n",
       "7  1304799112484370 2011-05-07 20:11:52.484370       0.05469        1   \n",
       "8  1304799112492180 2011-05-07 20:11:52.492180       0.06250        1   \n",
       "9  1304799112500000 2011-05-07 20:11:52.500000       0.07032        1   \n",
       "\n",
       "  trial_type  trial_num  trial_num_original trial_subtype  \\\n",
       "0       ADLs          1                   1            SQ   \n",
       "1       ADLs          1                   1            SQ   \n",
       "2       ADLs          1                   1            SQ   \n",
       "3       ADLs          1                   1            SQ   \n",
       "4       ADLs          1                   1            SQ   \n",
       "5       ADLs          1                   1            SQ   \n",
       "6       ADLs          1                   1            SQ   \n",
       "7       ADLs          1                   1            SQ   \n",
       "8       ADLs          1                   1            SQ   \n",
       "9       ADLs          1                   1            SQ   \n",
       "\n",
       "   r.ankle Acceleration X (m/s^2)  r.ankle Acceleration Y (m/s^2)   ...    \\\n",
       "0                       -9.691464                       -0.240769   ...     \n",
       "1                       -9.698196                       -0.245077   ...     \n",
       "2                       -9.707324                       -0.240524   ...     \n",
       "3                       -9.700565                       -0.238417   ...     \n",
       "4                       -9.698231                       -0.242868   ...     \n",
       "5                       -9.698263                       -0.240662   ...     \n",
       "6                       -9.705055                       -0.240563   ...     \n",
       "7                       -9.702853                       -0.236186   ...     \n",
       "8                       -9.700576                       -0.236234   ...     \n",
       "9                       -9.691576                       -0.234133   ...     \n",
       "\n",
       "   head resultant angular velocity  head resultant magnetic field  \\\n",
       "0                         0.205785                      83.535679   \n",
       "1                         0.215989                      83.284594   \n",
       "2                         0.215544                      84.138524   \n",
       "3                         0.208668                      82.744759   \n",
       "4                         0.225453                      82.618975   \n",
       "5                         0.218195                      81.709223   \n",
       "6                         0.219443                      80.866421   \n",
       "7                         0.216588                      82.462735   \n",
       "8                         0.218812                      82.890114   \n",
       "9                         0.217349                      82.744759   \n",
       "\n",
       "   sternum resultant acceleration  sternum resultant angular velocity  \\\n",
       "0                        9.735780                            0.265283   \n",
       "1                        9.736724                            0.272279   \n",
       "2                        9.748169                            0.269626   \n",
       "3                        9.746346                            0.266867   \n",
       "4                        9.738551                            0.269698   \n",
       "5                        9.736395                            0.279396   \n",
       "6                        9.731590                            0.274831   \n",
       "7                        9.738219                            0.279047   \n",
       "8                        9.741219                            0.266923   \n",
       "9                        9.732827                            0.274406   \n",
       "\n",
       "   sternum resultant magnetic field  waist resultant acceleration  \\\n",
       "0                         50.703732                      9.864789   \n",
       "1                         46.261673                      9.845113   \n",
       "2                         51.684996                      9.843324   \n",
       "3                         52.142097                      9.833984   \n",
       "4                         55.118507                      9.841990   \n",
       "5                         46.261673                      9.844404   \n",
       "6                         46.207206                      9.836417   \n",
       "7                         47.141498                      9.845819   \n",
       "8                         58.814823                      9.846862   \n",
       "9                         53.075897                      9.861802   \n",
       "\n",
       "   waist resultant angular velocity  waist resultant magnetic field  \\\n",
       "0                          1.122888                       57.649936   \n",
       "1                          1.129150                       53.942026   \n",
       "2                          1.121401                       49.053275   \n",
       "3                          1.125120                       56.348231   \n",
       "4                          1.133976                       55.478944   \n",
       "5                          1.121709                       57.873096   \n",
       "6                          1.123687                       60.276311   \n",
       "7                          1.111922                       52.042055   \n",
       "8                          1.124236                       53.510003   \n",
       "9                          1.122197                       53.643669   \n",
       "\n",
       "             FileName  target  \n",
       "0  JXL_SQ_trial1.xlsx       0  \n",
       "1  JXL_SQ_trial1.xlsx       0  \n",
       "2  JXL_SQ_trial1.xlsx       0  \n",
       "3  JXL_SQ_trial1.xlsx       0  \n",
       "4  JXL_SQ_trial1.xlsx       0  \n",
       "5  JXL_SQ_trial1.xlsx       0  \n",
       "6  JXL_SQ_trial1.xlsx       0  \n",
       "7  JXL_SQ_trial1.xlsx       0  \n",
       "8  JXL_SQ_trial1.xlsx       0  \n",
       "9  JXL_SQ_trial1.xlsx       0  \n",
       "\n",
       "[10 rows x 94 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full database\n",
    "dfMain = pickle.load(open(\"../../data/dataset_consolidated.p\", \"rb\"))\n",
    "dfMain.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data columns and separate them based on the sensor and the feature (accel, vel and magfield)\n",
    "\n",
    "allcols = dfMain.columns.values\n",
    "\n",
    "groupcols = ['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime']\n",
    "accelcols = []\n",
    "velcols = []\n",
    "magcols = []\n",
    "meancols = []\n",
    "resultantcols = []\n",
    "varcols = []\n",
    "\n",
    "for col in allcols : \n",
    "    if 'Velocity' in col :\n",
    "        velcols.append(col)\n",
    "    if 'Magnetic' in col :\n",
    "        magcols.append(col)\n",
    "    if 'Acceleration' in col :\n",
    "        accelcols.append(col)\n",
    "    if 'mean' in col :\n",
    "        meancols.append(col)\n",
    "    if 'var' in col :\n",
    "        varcols.append(col)\n",
    "    if 'resultant' in col :\n",
    "        resultantcols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use one of the following 4 cells depending on which section of the notebook you want to run unless your computer has enough memory to hold multiple copies of the ~1GB dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for the overlapping window preprocessing\n",
    "df = dfMain.copy()\n",
    "df = dfMain.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for the resultant peak window preprocessing\n",
    "df2 = dfMain.copy()\n",
    "df2 = df2.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for the resultant difference window preprocessing\n",
    "df3 = dfMain.copy()\n",
    "df3 = df3.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for the convolutional neural network approach\n",
    "dfConv = dfMain.copy()\n",
    "dfConv = dfConv.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overlapping windows around axis acceleration peaks\n",
    "#### Preprocessing steps : \n",
    "#### 1.1. Drop resultant columns since those aren't used in this method.\n",
    "#### 1.2. Group the data in 0.5 second windows and fine the mean and variance of each column for each window. (This reduces the size of the data to make it more manageable and also condenses the data rows to obtain more meaningful information out of them since originally each row is a set of sensor readings separated by ~0.00782 seconds each, which is very little time.\n",
    "#### 1.3. Pick the metrics and body parts to use on the model (waist acceleration only by default).\n",
    "#### 1.4. Pick a body part to create a fall window around (waist by default). Find the absolute peak acceleration mean (positive or negative) of each axis (X,Y,Z).\n",
    "#### 1.5. Everything from the smallest axis peak minus 2 seconds to the biggest axis peak plus 2 seconds will be considered a Fall (1) in fall trials, the rest of the observations will be 0s. \n",
    "#### 1.6. Separate the training and testing set (5 subjects for training, 5 for testing) and proceed to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b87b9aa1eff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop the resultant columns from here since this method doesn't use them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultantcols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop the resultant columns from here since this method doesn't use them\n",
    "df = df.drop(resultantcols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8b9f5bda5d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# group in intervals of 0.5 seconds, calculating the mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_window_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trial_type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trial_subtype'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trial_num'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trial_num_original'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time_datetime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'500000us'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_window_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_window_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# renaming the acceleration measurement columns, including a '_mean' in the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the mean\n",
    "df_window_mean = df.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).mean()\n",
    "df_window_mean = df_window_mean.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_mean' in the end\n",
    "for col in accelcols:\n",
    "    df_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)\n",
    "\n",
    "for col in velcols:\n",
    "    df_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)\n",
    "    \n",
    "for col in magcols:\n",
    "    df_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the variance\n",
    "\n",
    "df_window_variance = df.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).var()\n",
    "df_window_variance = df_window_variance.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_variance' in the end\n",
    "\n",
    "for col in accelcols : \n",
    "    df_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    \n",
    "for col in velcols : \n",
    "    df_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    \n",
    "for col in magcols : \n",
    "    df_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final dataframe, with all accelerometer columns (means and variances)\n",
    "all_trials_window = pd.merge(df_window_mean, df_window_variance,on=['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime'])\n",
    "\n",
    "# This dataframe will be used in case we decide to try different preprocessing steps\n",
    "all_trials_window = all_trials_window.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcols = all_trials_window.columns.values\n",
    "groupcols = ['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This following cell can be changed if you want to see the results combining different metrics/body parts, just add them to the lists as strings.\n",
    "\n",
    "#### If waist is removed from the bodyparts list, then some other cells might have to be changed in order to create the window around another body part. From the tests done there should be no reason to remove waist since it's one of the most important sensors, and creating the peak around other bodyparts doesn't produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = ['Acceleration'] # 'Acceleration','Velocity and/or 'Magnetic' can be added to this list\n",
    "bodyparts = ['waist'] # 'waist','sternum','head','r.ankle','l.ankle','r.thigh' and/or 'l.thigh' can be added to this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get just waist acceleration columns, time and groupcols\n",
    "dfWaistAccels = filterCols(all_trials_window,metrics,bodyparts,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "auxdf = dfWaistAccels\n",
    "\n",
    "# Add absolute value of the acceleration means as new columns to auxdf\n",
    "auxdf['AbsX'] = auxdf['waist Acceleration X (m/s^2)_mean'].abs()\n",
    "auxdf['AbsY'] = auxdf['waist Acceleration Y (m/s^2)_mean'].abs()\n",
    "auxdf['AbsZ'] = auxdf['waist Acceleration Z (m/s^2)_mean'].abs()\n",
    "\n",
    "# Find the id of the rows with max absolute value for each axis\n",
    "dfWaistAccels['YMax'] = auxdf.groupby(groupcols)['AbsY'].transform('idxmax')\n",
    "dfWaistAccels['XMax'] = auxdf.groupby(groupcols)['AbsX'].transform('idxmax')\n",
    "dfWaistAccels['ZMax'] = auxdf.groupby(groupcols)['AbsZ'].transform('idxmax')\n",
    "\n",
    "# Find the max and min ids from the last section\n",
    "dfWaistAccels['AxisMax'] = dfWaistAccels[[\"YMax\", \"XMax\",\"ZMax\"]].max(axis=1)\n",
    "dfWaistAccels['AxisMin'] = dfWaistAccels[[\"YMax\", \"XMax\",\"ZMax\"]].min(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "dfList = []\n",
    "for sub in dfWaistAccels['subject'].unique() :\n",
    "    for trialtype in dfWaistAccels['trial_type'].unique() :\n",
    "        for subtype in dfWaistAccels['trial_subtype'].unique() :\n",
    "            for num in dfWaistAccels['trial_num'].unique() :\n",
    "                aux1 = dfWaistAccels[(dfWaistAccels['subject'] == sub) & (dfWaistAccels['trial_type'] == trialtype) \n",
    "                    & (dfWaistAccels['trial_subtype'] == subtype) & (dfWaistAccels['trial_num'] == num)]\n",
    "                aux2 = aux1[(aux1.index < aux1.AxisMax+4) & (aux1.index > aux1.AxisMin-4)]\n",
    "                dfList.append(aux2)\n",
    "\n",
    "fulldf = pd.concat(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping the columns we don't need anymore\n",
    "fulldf = fulldf.drop(['XMax','ZMax','YMax','AxisMax','AxisMin','AbsX','AbsY','AbsZ'],axis=1)\n",
    "    \n",
    "fulldf['target'] = fulldf.apply (lambda row: generateTarget(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate train/test data, train the model and test it\n",
    "y_train = fulldf[(fulldf['subject'] >= 6)]['target']\n",
    "X_train = fulldf[(fulldf['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', 'trial_num', 'time_datetime','target'],axis=1)\n",
    "y_test = fulldf[(fulldf['subject'] < 6)]['target']\n",
    "X_test = fulldf[(fulldf['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', 'trial_num', 'time_datetime','target'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resultant peak windows :\n",
    "#### Preprocessing steps : \n",
    "#### 1.1. Group the data in 0.5 second windows and fine the mean and variance of each column for each window. (This reduces the size of the data to make it more manageable and also condenses the data rows to obtain more meaningful information out of them since originally each row is a set of sensor readings separated by ~0.00782 seconds each, which is very little time.\n",
    "#### 1.2. Pick the metrics and body parts to use on the model (waist acceleration only by default).\n",
    "#### 1.3. Initially add the target column as non-fall (0) for every observation.\n",
    "#### 1.4. Pick the body part to use to find the peak to create the window around. Find the peak resultant acceleration mean  of that body part and label the peak and the 4 second window around it as fall (meaning, 2 seconds before and 2 seconds after).\n",
    "#### 1.5. Separate the training and testing set (5 subjects for training, 5 for testing) and proceed to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the mean\n",
    "df2_window_mean = df2.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).mean()\n",
    "df2_window_mean = df2_window_mean.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_mean' in the end\n",
    "for col in df2.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df2_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the variance\n",
    "\n",
    "df2_window_variance = df2.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).var()\n",
    "df2_window_variance = df2_window_variance.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_variance' in the end\n",
    "for col in df2.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df2_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final dataframe, with all accelerometer columns (means and variances)\n",
    "df2_all_windows = pd.merge(df2_window_mean, df2_window_variance,on=['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime'])\n",
    "\n",
    "# This dataframe will be used in case we decide to try different preprocessing steps\n",
    "df2_all_windows = df2_all_windows.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This following cell can be changed if you want to see the results combining different metrics/body parts, just add them to the lists as strings.\n",
    "\n",
    "#### If waist is removed from the bodyparts list, then some other cells might have to be changed in order to create the window around another body part. From the tests done there should be no reason to remove waist since it's one of the most important sensors, and creating the peak around other bodyparts doesn't produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = ['Acceleration'] # 'Acceleration','Velocity and/or 'Magnetic' can be added to this list\n",
    "bodyparts = ['waist'] # 'waist','sternum','head','r.ankle','l.ankle','r.thigh' and/or 'l.thigh' can be added to this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfResWindows = filterCols(df2_all_windows,metrics,bodyparts,True)\n",
    "dfResWindows['target'] = dfResWindows.apply(lambda row: generateTarget2(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "df2List = []\n",
    "dropRows = False # True if you want to make the dataset more balanced by only getting the window around the peak \n",
    "# of each trial as the dataset (dropping any rows outside of each trial's respective peak)\n",
    "for sub in dfResWindows['subject'].unique() :\n",
    "    for trialtype in dfResWindows['trial_type'].unique() :\n",
    "        for subtype in dfResWindows['trial_subtype'].unique() :\n",
    "            for num in dfResWindows['trial_num'].unique() :\n",
    "                aux1 = dfResWindows[(dfResWindows['subject'] == sub) & \n",
    "                                         (dfResWindows['trial_type'] == trialtype) & \n",
    "                                         (dfResWindows['trial_subtype'] == subtype) & \n",
    "                                         (dfResWindows['trial_num'] == num)]\n",
    "                if (aux1.shape[0] > 0) :\n",
    "                    peak_index = aux1['waist resultant acceleration_mean'].idxmax()\n",
    "                    if (trialtype == 'Falls') :\n",
    "                        for i in range(peak_index-4,peak_index+4) : # Add the target 1 to the window\n",
    "                            aux1.set_value(i, 'target', 1)\n",
    "                    if (dropRows) :\n",
    "                        aux2 = aux1[(aux1.index < peak_index+4) & (aux1.index >= peak_index-4)]\n",
    "                        df2List.append(aux2)\n",
    "                    else : \n",
    "                        df2List.append(aux1)\n",
    "\n",
    "fulldf2 = pd.concat(df2List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      "TN : 7482 FP : 1259\n",
      "FN : 280 TP : 560\n",
      "\n",
      "Accuracy : 0.839369585638\n",
      "Recall : 0.666666666667\n",
      "Precision : 0.307861462342\n",
      "F-measure :0.421210981572\n",
      "Sensitivity : 66.6666666667\n",
      "Specificity : 85.5966136598\n"
     ]
    }
   ],
   "source": [
    "# Separate test/train data, fit and test the model\n",
    "oversampling = True\n",
    "train_positive = fulldf2[(fulldf2['subject'] >= 6) & (fulldf2['target'] == 1)] \n",
    "train_negetive = fulldf2[(fulldf2['subject'] >= 6) & (fulldf2['target'] == 0)] \n",
    "\n",
    "if oversampling is True:\n",
    "    # Uncomment for undersampling\n",
    "    #  training_data = pd.concat([train_negetive.sample(frac=1).head(train_positive.shape[0]), train_positive])\n",
    "\n",
    "    train_positive = pd.concat([train_positive, \n",
    "            train_positive.sample(n = int((train_negetive.shape[0] - train_positive.shape[0])), replace = True)])\n",
    "    # Uncomment for mix of over and undersampling\n",
    "    # train_negetive = train_negetive.sample(frac=1).head(int(train_positive.shape[0]))\n",
    "    training_data = pd.concat([train_negetive, train_positive])\n",
    "\n",
    "    training_data = training_data.sample(frac=1)\n",
    "    y_train = training_data['target']\n",
    "    X_train = training_data.drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "else:    \n",
    "    y_train = fulldf2[(fulldf2['subject'] >= 6)]['target']\n",
    "    X_train = fulldf2[(fulldf2['subject'] >= 6)].drop(['trial_num_original',\n",
    "                    'trial_type', 'subject', 'trial_subtype', \n",
    "                        'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "y_test = fulldf2[(fulldf2['subject'] < 6)]['target']\n",
    "X_test = fulldf2[(fulldf2['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "# Use the model below to use balanced class\n",
    "# clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=0.5, gamma=0.01, class_weight='balanced')\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to save the model and sample data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('./server/svm_model.pickle', 'wb'))\n",
    "X_train.to_csv('./server/server_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_server_runner_positive = fulldf2[(fulldf2['target'] == 1)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target', 'time_datetime'],axis=1).sample(frac=0.05)\n",
    "sample_server_runner_negetive = fulldf2[(fulldf2['target'] == 0)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target', 'time_datetime'],axis=1).sample(frac=0.05)\n",
    "\n",
    "\n",
    "# print(sample_server_runner_negetive.shape)\n",
    "clf.predict(sample_server_runner_positive.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Resultant Difference Window\n",
    "#### Preprocessing steps : \n",
    "#### 1.1. Group the data in 1.0 second windows and fine the mean and variance of each column for each window. (This reduces the size of the data to make it more manageable and also condenses the data rows to obtain more meaningful information out of them since originally each row is a set of sensor readings separated by ~0.00782 seconds each, which is very little time. The reason we use 1 second this time is because the calculation of the difference takes a long time and consumes a lot of memory, so having a smaller data set helps.\n",
    "#### 1.2. Pick the metrics and body parts to use on the model (waist acceleration only by default).\n",
    "#### 1.3. Initially add the target column as non-fall (0) for every observation.\n",
    "#### 1.4. Pick the body part to use to find the peak to create the window around. Calculate the difference in resultant acceleration mean between every 2 consecutive observations in the data. \n",
    "#### 1.5. Find the peak difference and label everything in a 4 second window around it as a fall (2 seconds before, 2 seconds after).\n",
    "#### 1.6. Separate the training and testing set (5 subjects for training, 5 for testing) and proceed to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group in intervals of 1.0 seconds, calculating the mean\n",
    "df3_window_mean = df3.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='1000000us')]).mean()\n",
    "df3_window_mean = df3_window_mean.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_mean' in the end\n",
    "for col in df3.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df3_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group in intervals of 1.0 seconds, calculating the variance\n",
    "\n",
    "df3_window_variance = df3.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='1000000us')]).var()\n",
    "df3_window_variance = df3_window_variance.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_variance' in the end\n",
    "for col in df3.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df3_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final dataframe, with all accelerometer columns (means and variances)\n",
    "df3_all_windows = pd.merge(df3_window_mean, df3_window_variance,on=['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime'])\n",
    "\n",
    "# This dataframe will be used in case we decide to try different preprocessing steps\n",
    "df3_all_windows = df3_all_windows.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This following cell can be changed if you want to see the results combining different metrics/body parts, just add them to the lists as strings.\n",
    "\n",
    "#### If waist is removed from the bodyparts list, then some other cells might have to be changed in order to create the window around another body part. From the tests done there should be no reason to remove waist since it's one of the most important sensors, and creating the peak around other bodyparts doesn't produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = ['Acceleration'] # 'Acceleration','Velocity and/or 'Magnetic' can be added to this list\n",
    "bodyparts = ['waist'] # 'waist','sternum','head','r.ankle','l.ankle','r.thigh' and/or 'l.thigh' can be added to this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3ResWindows = filterCols(df3_all_windows,metrics,bodyparts,True)\n",
    "df3ResWindows['target'] = df3ResWindows.apply(lambda row: generateTarget2(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the difference between every 2 consecutive observations for each trial\n",
    "df3List = []\n",
    "for sub in df3ResWindows['subject'].unique() :\n",
    "    for trialtype in df3ResWindows['trial_type'].unique() :\n",
    "        for subtype in df3ResWindows['trial_subtype'].unique() :\n",
    "            for num in df3ResWindows['trial_num'].unique() :\n",
    "                aux1 = df3ResWindows[(df3ResWindows['subject'] == sub) & \n",
    "                                         (df3ResWindows['trial_type'] == trialtype) & \n",
    "                                         (df3ResWindows['trial_subtype'] == subtype) & \n",
    "                                         (df3ResWindows['trial_num'] == num)]\n",
    "                aux1['resultant_diff'] = df3ResWindows['waist resultant acceleration_mean'].diff().fillna(0)\n",
    "                df3List.append(aux1)\n",
    "\n",
    "df3resdiff3 = pd.concat(df3List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "# This takes a while to run\n",
    "df3List = []\n",
    "dropRows = True\n",
    "for sub in df3resdiff3['subject'].unique() :\n",
    "    for trialtype in df3resdiff3['trial_type'].unique() :\n",
    "        for subtype in df3resdiff3['trial_subtype'].unique() :\n",
    "            for num in df3resdiff3['trial_num'].unique() :\n",
    "                aux1 = df3resdiff3[(df3resdiff3['subject'] == sub) & \n",
    "                                         (df3resdiff3['trial_type'] == trialtype) & \n",
    "                                         (df3resdiff3['trial_subtype'] == subtype) & \n",
    "                                         (df3resdiff3['trial_num'] == num)]\n",
    "                if (aux1.shape[0] > 0) :\n",
    "                    peak_index = aux1['resultant_diff'].idxmax()\n",
    "                    if (trialtype == 'Falls') :\n",
    "                        for i in range(peak_index-2,peak_index+2) : # Add the target 1 to the window\n",
    "                            aux1.set_value(i, 'target', 1)\n",
    "                    if (dropRows) :\n",
    "                        aux2 = aux1[(aux1.index < peak_index+2) & (aux1.index >= peak_index-2)]\n",
    "                        df3List.append(aux2) \n",
    "                    else :\n",
    "                        df3List.append(aux1)\n",
    "\n",
    "                    \n",
    "fulldf3 = pd.concat(df3List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate test/train data, fit and test the model\n",
    "y_train = fulldf3[(fulldf3['subject'] >= 6)]['target']\n",
    "X_train = fulldf3[(fulldf3['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "y_test = fulldf3[(fulldf3['subject'] < 6)]['target']\n",
    "X_test = fulldf3[(fulldf3['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convolutional Neural Networks\n",
    "#### Preprocessing steps : \n",
    "#### 1.1. This time we don't group the data in 0.5 or 1.0 second windows since we want the CNN to use the full dataset to try and find patterns in it. Condensing the data might cause a loss of patterns since ther eare roughly 125 observations for every second of trial.\n",
    "#### 1.2. Pick the metrics and body parts to use on the model (waist acceleration only by default).\n",
    "#### 1.3. Initially add the target column as non-fall (0) for every observation.\n",
    "#### 1.4. Pick the body part to use to find the peak to create the window around (Waist by default). Find the peak resultant acceleration of that body part and label the peak and the 4 second window around it as fall (meaning, 2 seconds before and 2 seconds after).\n",
    "#### 1.5. Separate the training and testing set (5 subjects for training, 5 for testing).\n",
    "#### 1.6. Transform the test and training sets into numpy arrays and reshape them to the format required by the Conv1D layer that Keras uses. Proceed to train and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This following cell can be changed if you want to see the results combining different metrics/body parts, just add them to the lists as strings.\n",
    "\n",
    "#### If waist is removed from the bodyparts list, then some other cells might have to be changed in order to create the window around another body part. From the tests done there should be no reason to remove waist since it's one of the most important sensors, and creating the peak around other bodyparts doesn't produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = ['Acceleration'] # 'Acceleration','Velocity and/or 'Magnetic' can be added to this list\n",
    "bodyparts = ['waist'] # 'waist','sternum','head','r.ankle','l.ankle','r.thigh' and/or 'l.thigh' can be added to this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop columns that we don't want\n",
    "df4 = filterCols(dfConv,metrics,bodyparts,True)\n",
    "# Set initial targets as 0s for every observation\n",
    "df4['target'] = df4.apply(lambda row: generateTarget2(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "df4List = []\n",
    "for sub in df4['subject'].unique() :\n",
    "    for trialtype in  df4['trial_type'].unique() :\n",
    "        for subtype in df4['trial_subtype'].unique() :\n",
    "            for num in df4['trial_num'].unique() :\n",
    "                aux1 = df4[(df4['subject'] == sub) & \n",
    "                                         (df4['trial_type'] == trialtype) & \n",
    "                                         (df4['trial_subtype'] == subtype) & \n",
    "                                         (df4['trial_num'] == num)]\n",
    "                if (aux1.shape[0] > 0) :\n",
    "                    if (trialtype == 'Falls') :\n",
    "                        peak_index = aux1['waist resultant acceleration'].idxmax()\n",
    "                        for i in range(peak_index-250,peak_index+250) : # Add the target 1 to the window\n",
    "                            aux1.set_value(i, 'target', 1)\n",
    "                    df4List.append(aux1)\n",
    "\n",
    "fulldf4 = pd.concat(df4List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate train and test set and drop useless columns\n",
    "y_train = fulldf4[(fulldf4['subject'] >= 6)]['target']\n",
    "X_train = fulldf4[(fulldf4['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "y_test = fulldf4[(fulldf4['subject'] < 6)]['target']\n",
    "X_test = fulldf4[(fulldf4['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform data into numpy arrays\n",
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape data to be able to use it with Conv1D\n",
    "X_train_final = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test_final = X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model.\n",
    "conv = Sequential()\n",
    "conv.add(Conv1D(filters=50, kernel_size=3, input_shape = (4,1), activation = 'relu'))\n",
    "conv.add(MaxPooling1D(2))\n",
    "conv.add(Flatten())\n",
    "conv.add(Dense(1, activation = 'sigmoid'))\n",
    "sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv.fit(X_train_final, y_train, batch_size = 4000, epochs = 500, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the shape of the inputs and outputs of each layer (No need to run this, just extra info)\n",
    "print(\"Model info : \")\n",
    "print(\"Input shape:  \"+str(X_train_final.shape))\n",
    "print(\"Conv1D Layer : \")\n",
    "print(\"Input : \" + str(conv.layers[0].input))\n",
    "print(\"Output : \"+str(conv.layers[0].output))\n",
    "print(\"MaxPooling1D Layer : \")\n",
    "print(\"Output : \"+str(conv.layers[1].output))\n",
    "print(\"Flatten Layer : \")\n",
    "print(\"Output : \"+str(conv.layers[2].output))\n",
    "print(\"Dense Output Layer :\")\n",
    "print(\"Output : \"+str(conv.layers[3].output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the input and output of each layer (No need to run this, just extra info)\n",
    "# Produces a lot of output\n",
    "\n",
    "# Looking at the output of 1 trial\n",
    "# Conv1D Layer\n",
    "get_layer_output1 = K.function([conv.layers[0].input],\n",
    "                                  [conv.layers[0].output])\n",
    "layer_output1 = get_layer_output1([X_test_final])[0]\n",
    "print(\"Conv1D Layer : (X filters for each of the observations)\")\n",
    "print(layer_output1[0])\n",
    "\n",
    "# MaxPooling1D Layer\n",
    "get_layer_output2 = K.function([conv.layers[1].input],\n",
    "                                  [conv.layers[1].output])\n",
    "layer_output2 = get_layer_output2([layer_output1])[0]\n",
    "print(\"MaxPooling1D Layer :\")\n",
    "print(layer_output2[0])\n",
    "# Flatten layer\n",
    "get_layer_output3 = K.function([conv.layers[2].input],\n",
    "                                  [conv.layers[2].output])\n",
    "layer_output3 = get_layer_output3([layer_output2])[0]\n",
    "print(\"Flatten Layer : \")\n",
    "print(layer_output3[0])\n",
    "# Dense layer\n",
    "get_layer_output4 = K.function([conv.layers[3].input],\n",
    "                                  [conv.layers[3].output])\n",
    "layer_output4 = get_layer_output4([layer_output3])[0]\n",
    "print(\"Dense Layer : \")\n",
    "print(layer_output4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model with the Keras built-in function\n",
    "metrics = conv.evaluate(X_test_final,y_test,verbose=0)\n",
    "print(\"Loss : \"+str(metrics[0]))\n",
    "print(\"Accuracy : \"+str(metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actually test the model to find the confusion matrix values\n",
    "preds = conv.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformedPreds = []\n",
    "for p in preds : \n",
    "    if (p >= 0.5) :\n",
    "        transformedPreds.append(1) \n",
    "    else :\n",
    "        transformedPreds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp,tn,fp,fn = 0,0,0,0\n",
    "for i in range(len(transformedPreds)) :\n",
    "    if ((transformedPreds[i] == y_test[i])) :\n",
    "        if (transformedPreds[i] == 1) and (y_test[i] == 1) :\n",
    "            tp += 1\n",
    "        if (transformedPreds[i] == 0) and (y_test[i] == 0) :\n",
    "            tn += 1\n",
    "    else :\n",
    "        if (transformedPreds[i] == 0) and (y_test[i] == 1) :\n",
    "            fn += 1\n",
    "        if (transformedPreds[i] == 1) and (y_test[i] == 0) :\n",
    "            fp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specificity = (tn / (fp + tn))*100\n",
    "sensitivity = (tp / (tp + fn))*100\n",
    "recall = (tp / (tp + fn))\n",
    "precision = (tp / (tp + fp)) \n",
    "accuracy = ((tn+tp) / (tp + tn + fp + fn))\n",
    "print(\"Confusion matrix : \")\n",
    "print(\"TN : \"+str(tn) + \" FP : \" +str(fp))\n",
    "print(\"FN : \"+str(fn) + \" TP : \" +str(tp))\n",
    "print(\"\")\n",
    "print(\"Accuracy : \"+str(accuracy))\n",
    "print(\"Sensitivity : \"+str(sensitivity))\n",
    "print(\"Specificity : \"+str(specificity))\n",
    "print(\"Precision : \"+str(precision))\n",
    "print(\"Recall : \"+str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results Visualization\n",
    "#### In this part we'll show the results of each set of tests done. In each barplot we're showing the results of 3 or 4 tests with their respective accuracies, recalls,presitions and specificities. The X axis indicates the body parts and metrics used (Head,Sternum,Ankles,Thighs  and/or Waist for body parts and Acceleration,Velocity and Magnetic field for metrics).\n",
    "\n",
    "#### Preprocessing 2 and 3 have a comment indicating \"dropping rows\" or \"not dropping rows\". The idea was to see the effects of only using the set window of data as our dataset and dropping the edges of each trial. Preprocessing 1 already does this so there was no reason to show the difference and the CNNs only work well using only Fall trials so there was no point in testing that.\n",
    "\n",
    "#### Preprocessing 3 only uses 3 tests due to the fact that using more than one body part causes the kernel to stop during the computation of the differences, so we only did 3 tests using accelerations of head, sternum and waist separately.\n",
    "\n",
    "#### The CNNs were tested using only fall trials vs all trials initially. The results for all trials are very underwhelming, so we decided to stick to only fall trials for this since the CNNs seem to be good enough at identifying falls inside different fall trials, but due to the way ADLs and Near-Falls work, the CNN most likely has a hard time picking up and learning patterns when all the trials are used, since some ADLs and Near-Falls are so similar to falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for preprocessing 1 :\n",
    "accL = np.array([0.782,0.761,0.711,0.662])\n",
    "recL =  np.array([0.428,0.431,0.237,0.0])\n",
    "precL = np.array([0.854,0.75,0.722,0.0])\n",
    "specL = np.array([0.962,0.930,0.953,1.0])\n",
    "xticks = np.array(['W/A','W,S,H/A','All/A','All/All'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for preprocessing 2 (without dropping rows) :\n",
    "accL = np.array([0.934,0.930,0.924,0.914])\n",
    "recL =  np.array([0.394,0.395,0.158,0.05])\n",
    "precL = np.array([0.743,0.680,0.869,0.646])\n",
    "specL = np.array([0.986,0.982,0.997,0.997])\n",
    "xticks = np.array(['W/A','S,H/A','W/V','H,Ts/A'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for preprocessing 2 (dropping rows ) :\n",
    "accL = np.array([0.801,0.776,0.674,0.708])\n",
    "recL =  np.array([0.567,0.566,0.139,0.2])\n",
    "precL = np.array([0.813,0.737,0.688,0.879])\n",
    "specL = np.array([0.929,0.890,0.965,0.985])\n",
    "xticks = np.array(['W/A','S,H/A','W/A,V,M','W,Ts/A'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for preprocessing 3 : (without dropping rows) \n",
    "accL = np.array([0.944,0.950,0.945])\n",
    "recL =  np.array([0.5,0.502,0.477])\n",
    "precL = np.array([0.763,0.854,0.796])\n",
    "specL = np.array([0.985,0.992,0.988])\n",
    "xticks = np.array(['W/A','H/A','S/V'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for preprocessing 3 : (dropping rows) \n",
    "accL = np.array([0.791,0.801,0.773])\n",
    "recL =  np.array([0.538,0.523,0.498])\n",
    "precL = np.array([0.807,0.856,0.779])\n",
    "specL = np.array([0.929,0.952,0.923])\n",
    "xticks = np.array(['W/A','H/A','S/V'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Conv neural networks : (Comparing falls only vs all trials)\n",
    "# all of them using only waist accelerations\n",
    "accL = np.array([0.787,0.912])\n",
    "recL =  np.array([0.566,0.268])\n",
    "precL = np.array([0.594,0.504])\n",
    "specL = np.array([0.864,0.974])\n",
    "xticks = np.array(['Falls500','All500'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Conv neural networks : (Using just falls, all with :\n",
    "# 50 filters, filter size 3, maxpooling=2 (gets the best element from the filter), 500 epochs : \n",
    "accL = np.array([0.787,0.711,0.801,0.787])\n",
    "recL =  np.array([0.566,0.737,0.678,0.725])\n",
    "precL = np.array([0.594,0.465,0.605,0.571])\n",
    "specL = np.array([0.864,0.703,0.844,0.809])\n",
    "xticks = np.array(['W/A','S,H,W/A','W,Ts/A','All/A'])\n",
    "plotRes(accL,recL,precL,specL,xticks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
